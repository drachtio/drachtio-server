[/
              Copyright Andrey Semashev 2007 - 2012.
     Distributed under the Boost Software License, Version 1.0.
        (See accompanying file LICENSE_1_0.txt or copy at
              http://www.boost.org/LICENSE_1_0.txt)

    This document is a part of Boost.Log library documentation.
/]

[section:extension Extending the library]

[section:sinks Writing your own sinks]

    #include <``[boost_log_sinks_basic_sink_backend_hpp]``>

As was described in the [link log.design Design overview] section, sinks consist of two parts: frontend and backend. Frontends are provided by the library and usually do not need to be reimplemented. Thanks to frontends, implementing backends is much easier than it could be: all filtering and thread synchronization is done there.

In order to develop a sink backend, you have two options where to start:

* If you don't need any formatting, the minimalistic [class_sinks_basic_sink_backend] base class template is your choice. Actually, this class only defines types that are needed for the sink to function.
* If you need to create a sink with formatting capabilities, you can use the [class_sinks_basic_formatting_sink_backend] class template as a base class for your backend. It extends the [class_sinks_basic_sink_backend] class and implements log record formatting and character code conversion, leaving you to develop only the record storing code.

Before we move on and see these instruments in action, one thing should be noted. As was said before, sink frontends take the thread safety burden from the backend. Also, there are [link log.detailed.sink_frontends three kinds of frontends] (not counting the ordering one). Each of them provides different guarantees regarding thread safety. The backend has no idea which sink frontend is used with it, yet it may require a certain degree of thread safety from it to function properly. In order to protect itself from misuse the backend declares the threading model it needs to operate with. There are three of them:

# The [class_sinks_backend_synchronization_tag] means that the backend itself is responsible for thread synchronization (which may imply there is no need for synchronization at all). When a backend declares this threading model, any sink frontend can be used with it.
# The [class_sinks_frontend_synchronization_tag] means that the frontend must serialize calls to the backend from different threads. The [class_sinks_unlocked_sink] frontend cannot fulfill this requirement, so it will not compile if instantiated with such a backend.
# The [class_sinks_single_thread_tag] means that all log records must be passed to the backend in a single thread. Note that other methods can be called in other threads, however, these calls must be serialized. Only [class_sinks_asynchronous_sink] and [class_sinks_ordering_asynchronous_sink] frontends meet this requirement, other frontends will refuse to compile with such a backend.

The threading model tag is used to instantiate the backend base classes. Since [class_sinks_basic_formatting_sink_backend] base class uses internal data to implement log record formatting, it requires the threading model to be either [class_sinks_frontend_synchronization_tag] or [class_sinks_single_thread_tag]. On the other hand, [class_sinks_basic_sink_backend] doesn't have this restriction.

[heading Minimalistic sink backend]

As an example of the [class_sinks_basic_sink_backend] class usage, let's implement a simple statistical information collector backend. Assume we have a network server and we want to monitor how many incoming connections are active and how much data was sent or received. The collected information should be written to a CSV-file every minute. The backend definition could look something like this:

    // The backend collects statistical information about network activity of the application
    class stat_collector :
        public sinks::basic_sink_backend<
            // Character type. We use narrow-character logging in this example.
            char,
            // We will have to store internal data, so let's require frontend to
            // synchronize calls to the backend.
            sinks::frontend_synchronization_tag
        >
    {
    private:
        // The file to write the collected information to
        std::ofstream m_CSVFile;

        // Here goes the data collected so far:
        // Active connections
        unsigned int m_ActiveConnections;
        // Sent bytes
        unsigned int m_SentBytes;
        // Received bytes
        unsigned int m_ReceivedBytes;

        // A thread that writes the statistical information to the file
        std::auto_ptr< boost::thread > m_WriterThread;

    public:
        // The function creates an instance of the sink
        template< template< typename > class FrontendT >
        static boost::shared_ptr< FrontendT< stat_collector > > create(const char* file_name);

        // The function consumes the log records that come from the frontend
        void consume(record_type const& rec);

    private:
        // The constructor initializes the internal data
        explicit stat_collector(const char* file_name) :
            m_CSVFile(file_name, std::ios::app),
            m_ActiveConnections(0)
        {
            reset_accumulators();
            if (!m_CSVFile.is_open())
                throw std::runtime_error("could not open the CSV file");
        }

        // The function runs in a separate thread and calls write_data periodically
        template< template< typename > class FrontendT >
        static void writer_thread(boost::weak_ptr< FrontendT< stat_collector > > const& sink);

        // The function resets statistical accumulators to initial values
        void reset_accumulators()
        {
            m_SentBytes = m_ReceivedBytes = 0;
        }

        // The function writes the collected data to the file
        void write_data()
        {
            m_CSVFile << m_ActiveConnections
                << ',' << m_SentBytes
                << ',' << m_ReceivedBytes
                << std::endl;
            reset_accumulators();
        }
    };

As you can see, the public interface of the backend is quite simple. In fact, only the `consume` function is needed by frontends, the `create` function is introduced for our own convenience. The `create` function simply creates the sink and initializes the thread that will write the collected data to the file.

    // The function creates an instance of the sink
    template< template< typename > class FrontendT >
    boost::shared_ptr< FrontendT< stat_collector > > stat_collector::create(const char* file_name)
    {
        // Create the backend
        boost::shared_ptr< stat_collector > backend(new stat_collector(file_name));

        // Wrap it into the specified frontend
        typedef FrontendT< stat_collector > sink_t;
        boost::shared_ptr< sink_t > sink(new sink_t(backend));

        // Now we can start the thread that writes the data to the file
        backend->m_WriterThread.reset(new boost::thread(
            &stat_collector::writer_thread< FrontendT >,
            boost::weak_ptr< FrontendT< stat_collector > >(sink)
        ));

        return sink;
    }

Now the `writer_thread` function can look like this:

    // The function runs in a separate thread and writes the collected data to the file
    template< template< typename > class FrontendT >
    void stat_collector::writer_thread(
        boost::weak_ptr< FrontendT< stat_collector > > const& sink)
    {
        while (true)
        {
            // Sleep for one minute
            boost::this_thread::sleep(boost::get_xtime(
                boost::get_system_time() + boost::posix_time::minutes(1)));

            // Get the pointer to the sink
            boost::shared_ptr< FrontendT< stat_collector > > p = sink.lock();
            if (p)
                p->locked_backend()->write_data(); // write the collected data to the file
            else
                break; // the sink is dead, terminate the thread
        }
    }

The `consume` function is called every time a logging record passes filtering in the frontend. The record, as was stated before, contains a set of attribute values and the message string.

Since we have no need for the record message, we will ignore it for now.

    // The function consumes the log records that come from the frontend
    void stat_collector::consume(record_type const& rec)
    {
        namespace lambda = boost::lambda;

        if (rec.attribute_values().count("Connected"))
            ++m_ActiveConnections;
        else if (rec.attribute_values().count("Disconnected"))
            --m_ActiveConnections;
        else
        {
            logging::extract< unsigned int >(
                "Sent",
                rec.attribute_values(),
                lambda::var(m_SentBytes) += lambda::_1);
            logging::extract< unsigned int >(
                "Received",
                rec.attribute_values(),
                lambda::var(m_ReceivedBytes) += lambda::_1);
        }
    }

The code above is quite straightforward. We can parse through attribute values like through a regular map, or use extractors with function objects to acquire individual values. __boost_lambda__ and similar libraries simplify generation of function objects that will receive the extracted value.

[heading Formatting sink backend]

As an example of a formatting sink backend, let's implement a sink that will emit events to a Windows event trace. Assume there's another process that will monitor these events and display them to the user as a balloon window in the notification area. The definition of such backend would look something like this:

    class event_notifier :
        public sinks::basic_formatting_sink_backend<
            // the "source" character type
            char,
            // the "target" character type
            // (optional, by default is the same as the source character type)
            wchar_t
        >
    {
        // A handle for the event provider
        REGHANDLE m_ProviderHandle;

    public:
        // Constructor. Initializes the event source handle.
        explicit event_notifier(CLSID const& provider_id)
        {
            if (EventRegister(&provider_id, NULL, NULL, &m_ProviderHandle) != ERROR_SUCCESS)
                throw std::runtime_error("Could not register event provider");
        }
        // Destructor. Unregisters the event source.
        ~event_notifier()
        {
            EventUnregister(m_ProviderHandle);
        }

        // The method puts the formatted message to the event trace
        void do_consume(record_type const& rec, target_string_type const& formatted_message);
    };

The [class_sinks_basic_formatting_sink_backend] class template is instantiated on two character types: the one that is used by the rest of logging system and the one that is required by the backend for further usage. Either of these types can be `char` or `wchar_t`. These character types may be the same, in which case the formatting is done without character conversion, pretty much equivalent to streaming attribute values into a regular `std::ostringstream`. In our case the underlying API requires wide strings, so we'll have to do character conversion while formatting. The conversion will be done according to the locale that is set up in the [class_sinks_basic_formatting_sink_backend] base class (see its `imbue` and `getloc` functions).

In order to differentiate the resulting string type from the string types used throughout the rest of logging library, the [class_sinks_basic_formatting_sink_backend] class defines the `target_string_type` type along with the standard `string_type`. In our case, `target_string_type` will contain wide characters, while `string_type` will be narrow.

The threading model of the sink backend can be specified as the third optional parameter of the [class_sinks_basic_formatting_sink_backend] class template. The default threading model is [class_sinks_frontend_synchronization_tag], which fits us just fine.

The [class_sinks_basic_formatting_sink_backend] base class implements just about everything that is required by the library from the backend. The only thing left is to implement the virtual `do_consume` method that receives the original log record and the result of formatting. In our case this method will pass the formatted message to the corresponding API:

    // The method puts the formatted message to the event log
    void event_notifier::do_consume(
        record_type const& rec, target_string_type const& formatted_message)
    {
        EventWriteString(
            m_ProviderHandle,
            WINEVENT_LEVEL_LOG_ALWAYS,
            0ULL /* keyword */,
            formatted_message.c_str());
    }

That's it. The example can be extended to make use of attribute values to fill other parameters, like event level and keywords mask. A more elaborate version of this example can be found in the library examples.

The resulting sink backend can be used similarly to other formatting sinks, like [class_sinks_basic_text_ostream_backend]:

    boost::shared_ptr< event_notifier > backend(new event_notifier(CLSID_MyNotifier));
    backend->set_formatter
    (
        fmt::stream
            << "[" << fmt::time("TimeStamp")
            << "] " << fmt::message()
    );

    typedef sinks::synchronous_sink< event_notifier > sink_t;
    boost::shared_ptr< sink_t > sink(new sink_t(backend));
    logging::core::get()->add_sink(sink);

[endsect]

[section:sources Writing your own sources]

    #include <``[boost_log_sources_threading_models_hpp]``>
    #include <``[boost_log_sources_basic_logger_hpp]``>

You can extend the library by developing your own sources and, for that matter, ways of collecting log data. Basically, you have two choices of how to start: you can either develop a new logger feature or design a whole new type of source. The first approach is good if all you need is to tweak the functionality of the existing loggers. The second approach is reasonable if the whole mechanism of collecting logs by the provided loggers is unsuitable for your needs.

[heading Creating a new logger feature]

Every logger provided by the library consists of a number of features that can be combined with each other. Each feature is responsible for a single and independent aspect of the logger functionality. For example, loggers that provide the ability to assign severity levels to logging records include the [class_sources_severity] feature. You can implement your own feature and use it along with the ones provided by the library.

A logger feature should follow these basic requirements:

* A logging feature should be a class template. It should have at least one template parameter type (let's name it `BaseT`).
* The feature must publicly derive from the `BaseT` template parameter.
* The feature must be default-constructible and copy-constructible.
* The feature must be constructible with a single argument of a templated type. The feature may not use this argument itself, but it should pass this argument to the `BaseT` constructor.

These requirements allow composition of a logger from a number of features derived from each other. The root class of the features hierarchy will be the [class_sources_basic_logger] class template instance. This class implements most of the basic functionality of loggers, like storing logger-specific attributes and providing the interface for log message formatting. The hierarchy composition is done by the [class_sources_basic_composite_logger] class template, which is instantiated on a sequence of features (don't worry, this will be shown in an example in a few moments). The constructor with a templated argument allows initializing features with named parameters, using the __boost_parameter__ library.

A logging feature may also contain internal data. In that case, to maintain thread safety for the logger, the feature should follow these additional guidelines:

# Usually there is no need to introduce a mutex or another synchronization mechanism in each feature. Moreover, it is advised not to do so, because the same feature can be used in both thread-safe and not thread-safe loggers. Instead, features should use the threading model of the logger as a synchronization primitive, similar to how they would use a mutex. The threading model is accessible through the `get_threading_model` method, defined in the [class_sources_basic_logger] class template.
# If the feature has to override `*_unlocked` methods of the protected interface of the [class_sources_basic_logger] class template (or the same part of the base feature interface), the following should be considered with regard to such methods:

    * The public methods that eventually call these methods are implemented by the [class_sources_basic_composite_logger] class template. These implementations do the necessary locking and then pass control to the corresponding `_unlocked` method of the base features.
    * The thread safety requirements for these methods are expressed with lock types. These types are available as typedefs in each feature and the [class_sources_basic_logger] class template. If the feature exposes a protected function `foo_unlocked`, it will also expose type `foo_lock`, which will express the locking requirements of `foo_unlocked`. The corresponding method `foo` in the [class_sources_basic_composite_logger] class template will use this typedef in order to lock the threading model before calling `foo_unlocked`.
    * Feature constructors don't need locking, and thus there's no need for lock types for them.

# The feature may implement a copy constructor. The argument of the constructor is already locked with a shared lock when the constructor is called. Naturally, the feature is expected to forward the copy constructor call to the `BaseT` class.
# The feature need not implement an assignment operator. The assignment will be automatically provided by the [class_sources_basic_composite_logger] class instance. However, the feature may provide a `swap_unlocked` method that will swap contents of this feature and the method argument, and call similar method in the `BaseT` class. The automatically generated assignment operator will use this method, along with copy constructor.

In order to illustrate all these lengthy recommendations, let's implement a simple logger feature. Suppose we want our logger to be able to tag individual log records. In other words, the logger has to temporarily add an attribute to its set of attributes, emit the logging record, and then automatically remove the attribute. Somewhat similar functionality can be achieved with scoped attributes, although the syntax may complicate wrapping it into a neat macro:

    // We want something equivalent to this
    {
        BOOST_LOG_SCOPED_LOGGER_TAG(logger, "Tag", std::string, "[GUI]");
        BOOST_LOG(logger) << "The user has confirmed his choice";
    }

Let's declare our logger feature:

[example_extension_record_tagger_declaration]

You can see that we use the [class_log_strictest_lock] template in order to define lock types that would fulfill the base class thread safety requirements for methods that are to be called from the corresponding methods of `record_tagger_feature`. The `open_record_lock` definition shows that the `open_record_unlocked` implementation for the `record_tagger_feature` feature requires exclusive lock (which `lock_guard` is) for the logger, but it also takes into account locking requirements of the `open_record_unlocked`, `add_attribute_unlocked` and `remove_attribute_unlocked` methods of the base class, because it will have to call them. The generated `open_record` method of the final logger class will make use of this typedef in order to automatically acquire the corresponding lock type before forwarding to the `open_record_unlocked` methods.

Actually, in this particular example, there was no need to use the [class_log_strictest_lock] trait, because all our methods require exclusive locking, which is already the strictest one. However, this template may come in handy, should you use shared locking.

The implementation of the public interface becomes quite trivial:

[example_extension_record_tagger_structors]

Now, since all locking is extracted into the public interface, we have the most of our feature logic to be implemented in the protected part of the interface. In order to set up tag value in the logger, we will have to introduce a new __boost_parameter__ keyword. Following recommendations from that library documentation, it's better to introduce the keyword in a special namespace:

[example_extension_record_tagger_keyword]

Opening a new record can now look something like this:

[example_extension_record_tagger_open_record]

Here we add a new attribute with the tag value, if one is specified in call to `open_record`. When a log record is opened, all attribute values are acquired and locked after the record, so we remove the tag from the attribute set with the __boost_scope_exit__ block.

Ok, we got our feature, and it's time to inject it into a logger. Assume we want to combine it with the standard severity level logging. No problems:

[example_extension_record_tagger_my_logger]

As you can see, creating a logger is a quite simple procedure. The `BOOST_LOG_FORWARD_LOGGER_MEMBERS_TEMPLATE` macro you see here is for mere convenience purpose: it unfolds into a default constructor, copy constructor, assignment operator and a number of constructors to support named arguments. For non-template loggers there is a similar `BOOST_LOG_FORWARD_LOGGER_MEMBERS` macro.

Assuming we have defined severity levels like this:

[example_extension_record_tagger_severity]

we can now use our logger as follows:

[example_extension_record_tagger_manual_logging]

All this verbosity is usually not required. One can define a special macro to make the code more concise:

[example_extension_record_tagger_macro_logging]

The complete code sample from this section is available in [@boost:/libs/log/example/doc/extension_record_tagger.cpp].

[heading Guidelines for designers of standalone logging sources]

In general, you can implement new logging sources the way you like, the library does not mandate any design requirements on log sources. However, there are some notes regarding the way log sources should interact with logging core.

# Whenever a logging source is ready to emit a log record, it should call the `open_record` in the corresponding core. The source-specific attributes should be passed into that call. During that call the core allocates resources for the record being made and performs filtering.
# If the call to `open_record` returned a valid log record, then the record passed the filtering and is considered to be opened. The record may later be either confirmed by the source by subsequently calling `push_record` or withdrawn by destroying it.
# If the call to `open_record` returned an invalid (empty) log record, it means that the record has not been opened (most likely due to filtering rejection). In that case the logging core does not hold any resources associated with the record, and thus the source must not call `push_record` for that particular logging attempt.
# The source may subsequently open more than one record. Opened log records exist independently from each other.
# The cores for different character types are completely independent. A log source would typically use one logging core with the appropriate character type.

[endsect]

[section:attributes Writing your own attributes]

    #include <``[boost_log_attributes_attribute_hpp]``>
    #include <``[boost_log_attributes_basic_attribute_value_hpp]``>

TODO: Update the description according to changes in the attribute value.

Developing your own attributes is quite simple. Generally, you need to do the following:

# Define what will be the attribute value. Most likely, it will be a piece of constant data that you want to participate in filtering and formatting. Encapsulate this data into a class that derives from the [class_log_attribute_value] interface. This object will have to implement the `dispatch` method that will extract the stored data (or, in other words, the stored value) to a type dispatcher.
# Define how attribute values are going to be produced. In a corner case the values do not need to be produced (like in the case of the [class_attributes_constant] attribute provided by the library), but often there is some logic that needs to be invoked to acquire the attribute value. This logic has to be concentrated in a class derived from the [class_log_attribute] interface, more precisely - in the `get_value` method. You can think of it as an attribute value factory.

While designing an attribute, one has to strive to make it as independent from the values it produces, as possible. The attribute can be called from different threads concurrently to produce a value. Once produced, the attribute value can be used several times by the library (maybe even concurrently), it can outlive the attribute object where it was created, and several attribute values produced by the same attribute can exist simultaneously.

Each attribute value is considered independent from other attribute values or the attribute itself, from the point of view of the library. That said, it is still possible to implement attributes that are also attribute values, and thus optimize performance. This is possible if it fulfils either of the following:

* The attribute value never changes, so it's possible to store it in the attribute itself. The [class_attributes_constant] attribute is an example.
* The attribute stores its value in a global (external with regard to the attribute) storage, that can be accessed from any attribute value. The attribute values must guarantee, though, that their stored values do not change over time.

As a special case for the second point, it is possible to store attribute values (or their parts) in thread-specific storage. However, in that case the user has to implement the `detach_from_thread` method of the attribute value properly. The result of this method - another attribute value - must be independent from the thread it is being called in, but its stored value should be equivalent to the original attribute value. This method will be called by the library when the attribute value passes to a thread that is different from the thread where it was created. As of this moment, this will only happen in the case of asynchronous logging sinks.

But in the vast majority of cases attribute values must be self-contained objects with no dependencies on other entities. In fact, this case is so common that the library provides a ready to use attribute value class template [class_attributes_basic_attribute_value]. The template has to be instantiated on the stored value type, and the stored value has to be provided to the attribute value constructor. For example, this is how to implement an attribute that will return system uptime in seconds:

    class system_uptime :
        public logging::attribute
    {
        typedef attrs::basic_attribute_value< unsigned int > attribute_value_type;

    public:
        boost::shared_ptr< attribute_value > get_value()
        {
            unsigned int up;

    #if defined(BOOST_WINDOWS)
            up = GetTickCount() / 1000;
    #else // assume other platforms provide sysinfo function
            struct sysinfo info;
            if (sysinfo(&info) != 0)
                throw std::runtime_error("Could not acquire uptime");
            up = info.uptime;
    #endif

            return boost::shared_ptr< attribute_value >(new attribute_value_type(up));
        }
    };

[endsect]

[section:settings Extending library settings support]

If you write your own logging sinks or use your own types in attributes, you may want to add support for these components to the settings parser provided by the library. Without doing this, the library will not be aware of your types and thus will not work properly.

[heading Adding support for user-defined types to the formatter parser]

    #include <``[boost_log_utility_init_formatter_parser_hpp]``>

In order to add support for user-defined types to the formatter parser, one has to register a formatter factory. The factory is basically a function object that, when called, will construct a formatter for the particular attribute. Factories are registered with the `register_formatter_factory` function, that besides the factory functor accepts the attribute name that will trigger this factory usage. This way the application can expose the knowledge of the particular attribute to the library. Here's a quick example:

    // Suppose, this class can be used as an attribute value
    struct Point
    {
        double m_X, m_Y;

        // Streaming operator
        template< typename CharT, typename TraitsT >
        friend std::basic_ostream< CharT, TraitsT >& operator<< (
            std::basic_ostream< CharT, TraitsT >& strm, Point const& point)
        {
            strm << "(" << point.m_X << ", " << point.m_Y << ")";
        }
    };

    // This is a helper traits that defines most of the types used by the formatter factories
    typedef logging::formatter_types< char > types;

    // Formatter factory
    types::formatter_type point_formatter_factory(
        types::string_type const& attr_name,
        types::formatter_factory_args const& args)
    {
        return types::formatter_type(fmt::attr< Point >(attr_name));
    }

    // We can associate the attribute with the name "Coordinates" with the type Point
    logging::register_formatter_factory("Coordinates", &point_formatter_factory);

Now, whenever the formatter parser (the `parse_formatter` function) encounters the "Coordinates" attribute in the format string being parsed, the `point_formatter_factory` will be called to construct the appropriate formatter. This formatter, since it is generated in the user's application, will use the custom streaming operator that is defined for the `Point` class.

The formatter factory can additionally accept a number of parameters separated with commas that can be specified in the format string. These parameters are broken into (name, value) pairs and passed as the second argument to the factory. For example, we could allow customizing the way our coordinates are presented in log by accepting an additional parameter in the format string like this:

[pre %TimeStamp% %Coordinates(format\="{%0.3f; %0.3f}")% %\_%]

Now in order to support this parameter we should rewrite our factory like this:

    namespace lambda = boost::lambda;

    // This formatter will use custom format string to format the point coordinates
    void custom_point_formatter(
        types::string_type const& attr_name,
        types::ostream_type& strm,
        types::record_type const& rec,
        types::string_type const& format)
    {
        Point point;
        if (logging::extract< Point >(
            attr_name,
            rec.attribute_values(),
            lambda::var(point) = lambda::_1))
        {
            // If the attribute set contains the needed attribute value,
            // format it into the stream
            strm << boost::format(format) % point.m_X % point.m_Y;
        }
    }

    // Formatter factory
    types::formatter_type point_formatter_factory(
        types::string_type const& attr_name,
        types::formatter_factory_args const& args)
    {
        types::formatter_factory_args::const_iterator it = args.find("format");
        if (it != args.end())
        {
            // The custom format is specified, use the special formatter
            return types::formatter_type(lambda::bind(
                &custom_point_formatter,
                attr_name,
                lambda::_1,
                lambda::_2,
                it->second));
        }
        else
        {
            // No special format specified, do things the traditional way
            return types::formatter_type(fmt::attr< Point >(attr_name));
        }
    }

However, if you don't need this additional flexibility and all you want is to use your custom streaming operators to format the attribute value, you can omit writing the formatter factory altogether. You can use a simple call like this:

    logging::register_simple_formatter_factory< Point >("Coordinates");

to achieve the same effect that the first version of the `point_formatter_factory` function provides.

[heading Adding support for user-defined types to the filter parser]

    #include <``[boost_log_utility_init_filter_parser_hpp]``>

You can extend filter parser the similar way you can extend the formatter parser - by registering your types into the library. However, since it takes a considerably more complex syntax to describe filters, a filter factory is more than a mere function.

Filter factories should be objects that derive from the [class_log_filter_factory] interface. This base class declares a number of virtual functions that will be called in order to create filters, according to the filter expression. If some functions are not overriden by the factory, the corresponding operations are considered to be not supported by the attribute value. For example, we can define the filter factory for the slightly improved `Point` class defined in the previous section the following way:

    // Suppose, this class can be used as an attribute value
    struct Point
    {
        double m_X, m_Y;

        // Comparison operators
        bool operator== (Point const& that) const;
        bool operator!= (Point const& that) const;

        // Streaming operators
        template< typename CharT, typename TraitsT >
        friend std::basic_ostream< CharT, TraitsT >& operator<< (
            std::basic_ostream< CharT, TraitsT >& strm, Point const& point);
        template< typename CharT, typename TraitsT >
        friend std::basic_istream< CharT, TraitsT >& operator>> (
            std::basic_istream< CharT, TraitsT >& strm, Point& point);
    };

    struct point_filter_factory :
        public logging::filter_factory< char >
    {
        // The callback for filter for the attribute existence test
        filter_type on_exists_test(string_type const& name)
        {
            return filter_type(flt::has_attr< Point >(name));
        }

        // The callback for equality relation filter
        filter_type on_equality_relation(string_type const& name, string_type const& arg)
        {
            return filter_type(flt::attr< Point >(name) == boost::lexical_cast< Point >(arg));
        }
        // The callback for inequality relation filter
        filter_type on_inequality_relation(string_type const& name, string_type const& arg)
        {
            return filter_type(flt::attr< Point >(name) != boost::lexical_cast< Point >(arg));
        }
    };

    // The factory can be registered in the following way
    logging::register_filter_factory("Coordinates", boost::make_shared< point_filter_factory >());

Having done that, whenever the filter parser (the `parse_filter` function) encounters the "Coordinates" attribute mentioned in the filter, it will use the `point_filter_factory` object to construct the appropriate filter. For example, in the case of the following filter

[pre %Coordinates% \= "(10, 10)"]

the `on_equality_relation` method will be called with `name` argument being "Coordinates" and `arg` being "10, 10".

[note The quotes around the parenthesis are necessary because the filter parser only supports binary relations, while round brackets are already used to group subexpressions of the filter expression. Whenever there is need to pass several parameters to the relation (like in this case - a number of components of the `Point` class) the parameters should be encoded into a quoted string. The string may include C-style escape sequences that will be unfolded upon parsing.]

The constructed filter will use the corresponding comparison operators for the `Point` class. Some relation operations, like ">" or "<=", will not be supported for attributes named "Coordinates", and this is just the way we want it, because the `Point` class does not support them either.

The library allows not only adding support for new types, but also associating new relations with them. For instance, we can create a new relation "is_in_rect" that will yield positive if the coordinates fit into a rectangle denoted with two points. The filter might look like this:

[pre %Coordinates% is\_in\_rect "(10, 10) - (20, 20)"]

To support it one has to define the `on_custom_relation` method in the filter factory:

    namespace bll = boost::lambda;

    struct Rectangle
    {
        Point m_TopLeft, m_BottomRight;

        // Streaming operators
        template< typename CharT, typename TraitsT >
        friend std::basic_ostream< CharT, TraitsT >& operator<< (
            std::basic_ostream< CharT, TraitsT >& strm, Rectangle const& rect);
        template< typename CharT, typename TraitsT >
        friend std::basic_istream< CharT, TraitsT >& operator>> (
            std::basic_istream< CharT, TraitsT >& strm, Rectangle& rect);
    };

    // Our custom filter type
    class is_in_rect_filter :
        public flt::basic_filter< char, is_in_rect_filter >
    {
    private:
        string_type m_Name;
        Rectangle m_Rect;

    public:
        is_in_rect_filter(string_type const& attr_name, Rectangle const& rect) :
            m_Name(attr_name),
            m_Rect(rect)
        {
        }

        bool operator() (values_view_type const& attrs) const
        {
            Point point;
            if (logging::extract< Point >(m_Name, attrs, bll::var(point) = bll::_1))
            {
                // Check that the point fits into the rectangle region
                return point.m_X >= m_Rect.m_TopLeft.m_X && point.m_X <= m_Rect.m_BottomRight.m_X
                    && point.m_Y >= m_Rect.m_TopLeft.m_Y && point.m_Y <= m_Rect.m_BottomRight.m_Y;
            }
            else
                return false;
        }
    };

    struct point_filter_factory :
        public logging::filter_factory< char >
    {
        // The callback for custom relation filter
        filter_type on_custom_relation(
            string_type const& name, string_type const& rel, string_type const& arg)
        {
            if (rel == "is_in_rect")
            {
                // Parse the coordinates of the rectangle region and construct the filter
                return filter_type(is_in_rect_filter(name, boost::lexical_cast< Rectangle >(arg));
            }
            else
                throw std::runtime_error("Relation " + rel + " is not supported");
        }
    };

Like with formatters, if all these bells and whistles are not needed, user can register a trivial filter factory with a simple call:

    logging::register_simple_filter_factory< Point >("Coordinates");

In this case, however, the `Point` class has to support all the standard relational operations and have appropriate streaming operators in order to be parsed from a string.

[heading Adding support for user-defined sinks]

    #include <``[boost_log_utility_init_from_stream_hpp]``>

The library provides mechanism of extending support for sinks similar to the formatter and filter parsers. In order to be able to mention user-defined sinks in a settings file, the user has to register a sink factory, which is essentially a function object that receives a number of named parameters and returns a pointer to the initialized sink. The factory is registered for a specific destination (see the [link log.detailed.utilities.init.settings settings file description]), so whenever a sink with the specified destination is mentioned in the settings file, the factory gets called. For instance, if we have a sink that emits SNMP traps as a result of processing log records, we can register it the following way:

    class snmp_backend :
        public sinks::basic_sink_backend< char, sinks::frontend_synchronization_tag >
    {
    public:
        // The constructor takes an address of the receiver of the traps
        explicit snmp_backend(std::string const& trap_receiver);

        // The function consumes the log records that come from the frontend and emits SNMP traps
        void consume(record_type const& rec);
    };

    // Factory function for the SNMP sink
    boost::shared_ptr< sinks::sink< char > > create_snmp_sink(
        std::map< std::string, std::string > const& params)
    {
        // Read parameters for the backend and create it
        std::map< std::string, std::string >::const_iterator it = params.find("TrapReceiver");
        if (it == params.end())
            throw std::runtime_error("TrapReceiver parameter not specified for the SNMP backend");

        boost::shared_ptr< snmp_backend > backend =
            boost::make_shared< snmp_backend >(it->second);

        // Construct and initialize the final sink
        typedef sinks::synchronous_sink< snmp_backend > sink_t;
        boost::shared_ptr< sink_t > sink =
            boost::make_shared< sink_t >(backend);

        it = params.find("Filter");
        if (it != params.end())
            sink->set_filter(logging::parse_filter(it->second));

        return sink;
    }

    logging::register_sink_factory("SNMP", &create_snmp_sink);

Now the SNMP sink can be constructed with the following settings:

[pre
\[Sink:MySNMPSink\]

Destination\=SNMP
Filter\="%Severity% > 3"
]

[tip Although users are free to name parameters of their sinks the way they like, a good choice would be to follow the naming policy established by the library. That is, it should be obvious that the parameter "Filter" means the same for both the library-provided "TextFile" sink and your custom "SNMP" sink backend.]

[note As the "Destination" parameter is used to determine the sink factory, this parameter is reserved and cannot be used by sink factories for their own purposes.]

[endsect]

[endsect]
